import streamlit as st
import google.generativeai as genai

# --- 1. CONFIGURACIÃ“N ---
st.set_page_config(page_title="Asesor CATEM", page_icon="âš–ï¸", layout="centered", initial_sidebar_state="expanded")

# --- 2. ESTILOS ---
st.markdown("""
    <style>
    h1 { color: #B71C1C !important; text-align: center; }
    .stChatMessage { border-radius: 15px; border: 1px solid #E0E0E0; }
    </style>
""", unsafe_allow_html=True)

# --- 3. SIDEBAR ---
with st.sidebar:
    st.header("ğŸ§° Herramientas")
    if st.button("ğŸ—‘ï¸ Nueva Consulta", type="primary"):
        st.session_state.messages = []
        st.rerun()
    st.divider()
    st.success("âœ… **Sistema Operativo**")
    st.info("âš¡ **Modelo:** Flash (AutomÃ¡tico)")
    st.warning("âš ï¸ Demo educativa.")

# --- 4. CONEXIÃ“N (USANDO EL COMODÃN DE TU LISTA) ---
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    
    # Â¡AQUÃ ESTÃ LA CLAVE! Usamos el alias genÃ©rico de tu lista
    model = genai.GenerativeModel('gemini-flash-latest', system_instruction="""
    Eres el "Asesor Digital CATEM".
    ROL: IA experta en derecho laboral mexicano (LFT).
    TONO: EmpÃ¡tico, profesional y firme.
    REGLAS:
    - Di siempre que NO eres abogado humano.
    - Despidos: Advierte NO firmar renuncia.
    - Usa negritas para resaltar claves.
    """)
except Exception as e:
    st.error(f"Error de conexiÃ³n: {e}")

# --- 5. CHAT ---
st.title("âš–ï¸ Asesor Digital CATEM")
st.markdown("<h3 style='text-align: center; color: #666;'>Tu aliado en la defensa laboral</h3>", unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

if len(st.session_state.messages) == 0:
    st.chat_message("assistant").write("Â¡Hola compaÃ±ero! ğŸ‘· Soy tu Asesor Virtual. Â¿Te despidieron o tienes dudas de tu salario? CuÃ©ntame.")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Escribe tu situaciÃ³n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        with st.spinner("Analizando..."):
            try:
                response = model.generate_content(prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "model", "content": response.text})
            except Exception as e:
                # Si falla, mostramos el mensaje amigable
                st.error("El sistema estÃ¡ saturado. Por favor espera 30 segundos e intenta de nuevo.")
